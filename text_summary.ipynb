{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZBxe1syeeUR",
        "outputId": "ac567fc8-edd9-4cd1-a61c-4cde778aa407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "2022-09-03 08:39:25.265319: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.0/en_core_web_lg-3.4.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 587.7 MB 8.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (21.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "# !pip install -U spacy\n",
        "# !python -m spacy download en_core_web_lg\n",
        "# # !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation"
      ],
      "metadata": {
        "id": "HvsTl5lNe5zD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = list(STOP_WORDS)\n",
        "print(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVYUDcJpf40-",
        "outputId": "8a312c64-8707-4df3-c05d-139258107883"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['last', 'may', 'between', '‘ve', 'which', 'hereby', 'why', 'five', 'hereupon', 'wherein', 'mine', 'whole', 'than', 'have', 'upon', 'without', 'toward', 'own', 'that', 'its', 'out', 'what', 'throughout', 'six', 'whatever', 'whereupon', 'done', 'somewhere', 'doing', 'enough', 'wherever', 'nor', 'mostly', 'those', 'hers', 'whereby', 'herself', 'any', 'his', 'once', 'be', 'anyhow', 'meanwhile', 'twenty', 'indeed', 'yet', 'whence', 'perhaps', 'are', 'where', 'something', 'a', 'noone', 'to', 'less', 'you', \"'m\", 'go', 'if', 'front', 'among', 'anywhere', 'has', 'three', 'therefore', 'first', 'other', 'sixty', 'everywhere', 'per', '‘m', 'thus', 'it', 'nothing', 'sometimes', 'much', 'within', 'been', 'whom', 'further', '’ve', 'otherwise', 'nine', 'whenever', 'formerly', 'along', 'is', 'my', 'cannot', 'towards', 'at', 'yourselves', 'very', '‘re', 'most', 'who', \"'re\", 'from', 'bottom', '‘s', 'latterly', 'twelve', 'regarding', 'none', 'same', 'please', 'thereafter', 'forty', '’s', 'via', 'namely', 'however', 'often', 'only', 'another', 'next', 'besides', 'full', 'hereafter', 'say', 'part', 'am', 'had', 'myself', 'so', '’ll', 'no', 'even', 'almost', 'again', 'elsewhere', 'beyond', 'the', 'made', 'hundred', 'will', 'might', 'while', '’d', 'several', 'quite', 'this', 'every', 'neither', 'many', 'each', 'put', 'some', 'beside', 'latter', 'beforehand', 'whoever', 'because', 'yourself', 'whose', 'more', 'ever', 'below', 'their', 'although', \"'ll\", 'fifty', 'alone', 'can', 'though', 'give', 'thereby', 'seems', 'serious', 'against', 'an', 'being', 'side', 'see', 'until', 'as', 'then', 'now', 'after', 'or', 'n’t', 'except', 'was', 'by', 'i', 'of', 'about', 'both', 'amount', 'such', 'onto', 'top', 'up', 'still', 'get', '’m', 'using', 'our', 'anyway', 'never', \"'s\", 'unless', 'and', 'move', 'thence', 'under', 'should', 'did', '‘d', 'becoming', 'does', 'various', 'on', 'afterwards', 'eleven', 'themselves', 'around', 'ca', 'could', 'anyone', 'really', 'in', 'them', 'when', 'few', 'into', 'whither', 'us', 'whereafter', 'third', 'she', \"'ve\", 'herein', 'n‘t', 'either', 'least', 'too', 'ourselves', 'also', 'used', 'become', 'fifteen', 'whereas', 'empty', 'always', 'together', 'ten', 'everyone', 'but', '’re', 'seem', 'her', 'moreover', 'just', 'with', 'thereupon', 'across', 'we', 'these', 'above', 'for', 'former', 'how', 'eight', 'there', 'anything', 'four', 're', 'keep', 'off', 'everything', 'take', 'behind', 'sometime', 'were', 'others', \"'d\", 'one', 'due', 'became', 'two', 'your', 'yours', 'well', 'not', 'itself', 'himself', '‘ll', 'somehow', 'during', 'thru', 'since', 'becomes', 'else', 'already', 'seemed', 'me', 'nevertheless', 'through', 'name', 'they', 'hence', 'do', 'back', 'all', 'would', 'must', 'over', 'ours', 'nobody', 'before', 'seeming', 'therein', 'he', 'whether', 'down', 'call', 'someone', 'amongst', 'him', 'rather', 'show', 'make', 'here', \"n't\", 'nowhere']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"\"\"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
        "\n",
        "Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "pOYEaHgqg2IY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "metadata": {
        "id": "ssoYyI8GhJ_s"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "UAfzwUnfhQwl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-zOmJ_thceV",
        "outputId": "11e2b5d9-4ece-4f4f-b59a-2f98822194ae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.', 'The', 'goal', 'is', 'a', 'computer', 'capable', 'of', '\"', 'understanding', '\"', 'the', 'contents', 'of', 'documents', ',', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them', '.', 'The', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves', '.', '\\n\\n', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'speech', 'recognition', ',', 'natural', '-', 'language', 'understanding', ',', 'and', 'natural', '-', 'language', 'generation', '.', '\\n\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation= punctuation+'\\n'"
      ],
      "metadata": {
        "id": "c_zHJch0h7C9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq = {}\n",
        "for word in doc:\n",
        "   if word.text.lower() not in stopwords:\n",
        "     if word.text.lower() not in punctuation:\n",
        "       if word.text not in word_freq.keys():\n",
        "         word_freq[word.text]=1\n",
        "       else:\n",
        "        word_freq[word.text]+=1\n",
        "word_freq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKotskPFiBxm",
        "outputId": "0255cd95-74b1-4199-c239-1a0927578861"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Natural': 1,\n",
              " 'language': 7,\n",
              " 'processing': 2,\n",
              " 'NLP': 1,\n",
              " 'subfield': 1,\n",
              " 'linguistics': 1,\n",
              " 'computer': 2,\n",
              " 'science': 1,\n",
              " 'artificial': 1,\n",
              " 'intelligence': 1,\n",
              " 'concerned': 1,\n",
              " 'interactions': 1,\n",
              " 'computers': 2,\n",
              " 'human': 1,\n",
              " 'particular': 1,\n",
              " 'program': 1,\n",
              " 'process': 1,\n",
              " 'analyze': 1,\n",
              " 'large': 1,\n",
              " 'amounts': 1,\n",
              " 'natural': 4,\n",
              " 'data': 1,\n",
              " 'goal': 1,\n",
              " 'capable': 1,\n",
              " 'understanding': 2,\n",
              " 'contents': 1,\n",
              " 'documents': 3,\n",
              " 'including': 1,\n",
              " 'contextual': 1,\n",
              " 'nuances': 1,\n",
              " 'technology': 1,\n",
              " 'accurately': 1,\n",
              " 'extract': 1,\n",
              " 'information': 1,\n",
              " 'insights': 1,\n",
              " 'contained': 1,\n",
              " 'categorize': 1,\n",
              " 'organize': 1,\n",
              " '\\n\\n': 2,\n",
              " 'Challenges': 1,\n",
              " 'frequently': 1,\n",
              " 'involve': 1,\n",
              " 'speech': 1,\n",
              " 'recognition': 1,\n",
              " 'generation': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_freq= max(word_freq.values())\n",
        "max_freq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkDTTmE4jj6s",
        "outputId": "b65a4581-34ea-4785-fb9c-28b318585ec5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word_freq.keys():\n",
        "  word_freq[word]= word_freq[word]/max_freq"
      ],
      "metadata": {
        "id": "2zcqM2J3jwUR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA8genRxkAXr",
        "outputId": "878a3111-63d9-4f2f-c5ab-cfed16c3a03b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Natural': 0.14285714285714285, 'language': 1.0, 'processing': 0.2857142857142857, 'NLP': 0.14285714285714285, 'subfield': 0.14285714285714285, 'linguistics': 0.14285714285714285, 'computer': 0.2857142857142857, 'science': 0.14285714285714285, 'artificial': 0.14285714285714285, 'intelligence': 0.14285714285714285, 'concerned': 0.14285714285714285, 'interactions': 0.14285714285714285, 'computers': 0.2857142857142857, 'human': 0.14285714285714285, 'particular': 0.14285714285714285, 'program': 0.14285714285714285, 'process': 0.14285714285714285, 'analyze': 0.14285714285714285, 'large': 0.14285714285714285, 'amounts': 0.14285714285714285, 'natural': 0.5714285714285714, 'data': 0.14285714285714285, 'goal': 0.14285714285714285, 'capable': 0.14285714285714285, 'understanding': 0.2857142857142857, 'contents': 0.14285714285714285, 'documents': 0.42857142857142855, 'including': 0.14285714285714285, 'contextual': 0.14285714285714285, 'nuances': 0.14285714285714285, 'technology': 0.14285714285714285, 'accurately': 0.14285714285714285, 'extract': 0.14285714285714285, 'information': 0.14285714285714285, 'insights': 0.14285714285714285, 'contained': 0.14285714285714285, 'categorize': 0.14285714285714285, 'organize': 0.14285714285714285, '\\n\\n': 0.2857142857142857, 'Challenges': 0.14285714285714285, 'frequently': 0.14285714285714285, 'involve': 0.14285714285714285, 'speech': 0.14285714285714285, 'recognition': 0.14285714285714285, 'generation': 0.14285714285714285}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sen_tokens = [sent for sent in doc.sents]\n",
        "sen_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohEKwBZ1kRFz",
        "outputId": "1739b086-6122-40c0-bd19-f1b2284e354a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.,\n",
              " The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.,\n",
              " The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
              " ,\n",
              " Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.\n",
              " ]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_score = {}\n",
        "for sent in sen_tokens:\n",
        "  for word in  sent:\n",
        "    if word.text.lower() in word_freq.keys():\n",
        "      if sent not in sent_score.keys():\n",
        "        sent_score[sent]= word_freq[word.text.lower()]\n",
        "      else:\n",
        "        sent_score[sent]= word_freq[word.text.lower()]\n",
        "        "
      ],
      "metadata": {
        "id": "6U2HSYJDkqDG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5v2z4nVvAhj",
        "outputId": "940145fa-64bc-4b78-889f-80bac02f6f97"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.: 0.14285714285714285,\n",
              " The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.: 1.0,\n",
              " The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
              " : 0.2857142857142857,\n",
              " Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.\n",
              " : 0.2857142857142857}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from heapq import nlargest"
      ],
      "metadata": {
        "id": "xPmTgm2uvNC3"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "C63k78UTvXT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "select_len =int(len(sent_score)*0.3)"
      ],
      "metadata": {
        "id": "t7BIIo7avR_o"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "select_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzZGkkbOvdxy",
        "outputId": "d2ac030c-361a-41df-b2cb-23c8d181f6af"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary= nlargest(select_len,sent_score,key=sent_score.get)"
      ],
      "metadata": {
        "id": "B8zDGnkUviwj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZhLSImEvvAG",
        "outputId": "c92a7a6d-2b8b-4f91-d040-9a517dfc8e96"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_sum={word.text for word in summary}\n",
        "final_sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2VnQTDAv4O0",
        "outputId": "c3fad0b1-161c-4e9b-fe69-0569340f2610"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = ' '.join(final_sum)"
      ],
      "metadata": {
        "id": "MT278xKJwJM8"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gMjvXVmwPFe",
        "outputId": "7da57e38-240f-4cf8-9a93-9c9c20d6728c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.\n"
          ]
        }
      ]
    }
  ]
}